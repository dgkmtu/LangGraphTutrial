{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do3oRsd1alpi"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne63TGfAZZlD"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-community langchain langchain-openai langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX09omtmUoQ_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# データベース作成＆接続\n",
        "conn = sqlite3.connect(\"knowledge.db\")\n",
        "c = conn.cursor()\n",
        "\n",
        "# ナレッジテーブル作成\n",
        "c.execute('''\n",
        "CREATE TABLE IF NOT EXISTS knowledge (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    title TEXT,\n",
        "    content TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "# 初期ナレッジを挿入\n",
        "c.executemany('''\n",
        "INSERT INTO knowledge (title, content) VALUES (?, ?)\n",
        "''', [\n",
        "    (\"LangChainとは\", \"LangChainはLLMのチェーン構築フレームワークです。\"),\n",
        "    (\"RAGとは\", \"RAGはRetrieval-Augmented Generationの略で、検索と生成を組み合わせます。\")\n",
        "])\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "V0PF0S9tS9xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAQmd-PyU_l9"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import TypedDict, Literal, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X91vSRFmVkwM"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    input: str\n",
        "    intent: Optional[Literal[\"rag\", \"llm\"]]\n",
        "    response: Optional[str]\n",
        "    quality_ok: Optional[bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9JHq6WNcJ_3"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSqSkAXoZIM-"
      },
      "outputs": [],
      "source": [
        "# SQLiteからデータ取得\n",
        "conn = sqlite3.connect(\"knowledge.db\")\n",
        "c = conn.cursor()\n",
        "rows = c.execute(\"SELECT title, content FROM knowledge\").fetchall()\n",
        "conn.close()\n",
        "\n",
        "# LangChain用ドキュメントに変換\n",
        "documents = [Document(page_content=f\"{title}\\n{content}\") for title, content in rows]\n",
        "\n",
        "# 分割してベクトル化\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# ベクトルストア作成\n",
        "vectorstore = Chroma.from_documents(docs, embedding, persist_directory=\"./chroma_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DH0_zElVvZf"
      },
      "outputs": [],
      "source": [
        "# ノード定義\n",
        "def classify_intent(state):\n",
        "  query = state[\"input\"]\n",
        "  if \"RAG\" in query:\n",
        "    state[\"intent\"] = \"rag\"\n",
        "  else:\n",
        "    state[\"intent\"] = \"llm\"\n",
        "  return state\n",
        "\n",
        "def rag_node(state):\n",
        "  query = state[\"input\"]\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "  retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "  # もっとも関連性の高い文書を返却\n",
        "  retrieved_text = (\n",
        "    retrieved_docs[0].page_content if retrieved_docs else \"該当する情報が見つかりませんでした。\"\n",
        "  )\n",
        "  return {\"response\": f\"[RAG] {retrieved_text}\"}\n",
        "\n",
        "def llm_node(state):\n",
        "  user_input = state.get(\"input\", \"\")\n",
        "  messages = [HumanMessage(content=user_input)]\n",
        "  response = llm.invoke(messages)\n",
        "  answer = response.content\n",
        "\n",
        "  # LLMの出力をナレッジベースに追加\n",
        "  conn = sqlite3.connect(\"knowledge.db\")\n",
        "  c = conn.cursor()\n",
        "  c.execute(\"INSERT INTO knowledge (title, content) VALUES (?, ?)\", (user_input, answer))\n",
        "  conn.commit()\n",
        "  conn.close()\n",
        "\n",
        "  return {\"response\": answer}\n",
        "\n",
        "def evaluate_answer(state):\n",
        "  resp = state[\"response\"]\n",
        "  if \" (\" in resp:\n",
        "    # 品質低いと見なす\n",
        "    state[\"quality_ok\"] = False\n",
        "  else:\n",
        "    state[\"quality_ok\"] = True\n",
        "  return state\n",
        "\n",
        "def loop_or_end(state):\n",
        "  return \"repeat\" if not state[\"quality_ok\"] else \"end\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwA_8cZWXL4a"
      },
      "outputs": [],
      "source": [
        "# --- グラフ構築 ---\n",
        "workflow = StateGraph(State)\n",
        "workflow.set_entry_point(\"classify\")\n",
        "workflow.add_node(\"classify\", RunnableLambda(classify_intent))\n",
        "workflow.add_node(\"rag\", RunnableLambda(rag_node))\n",
        "workflow.add_node(\"llm\", RunnableLambda(llm_node))\n",
        "workflow.add_node(\"evaluate\", RunnableLambda(evaluate_answer))\n",
        "\n",
        "# 分岐と遷移\n",
        "workflow.add_conditional_edges(\"classify\", lambda s: s[\"intent\"], {\n",
        "    \"rag\": \"rag\",\n",
        "    \"llm\": \"llm\"\n",
        "})\n",
        "workflow.add_edge(\"rag\", \"evaluate\")\n",
        "workflow.add_edge(\"llm\", \"evaluate\")\n",
        "workflow.add_conditional_edges(\"evaluate\", loop_or_end, {\n",
        "    \"classify\": \"classify\",\n",
        "    \"end\": END\n",
        "})\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvZ4K0LAYVMD"
      },
      "outputs": [],
      "source": [
        "input_state = {\"input\": \"RAGについて説明して。\"}\n",
        "for s in app.stream(input_state):\n",
        "  print(s)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なパッケージをインストール\n",
        "!apt-get install -y graphviz libgraphviz-dev pkg-config\n",
        "!pip install pygraphviz"
      ],
      "metadata": {
        "id": "nO2aqUaxfSx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# グラフの描画\n",
        "Image(app.get_graph().draw_png())"
      ],
      "metadata": {
        "id": "Zr624t-8fXAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6mOj2qGffYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}